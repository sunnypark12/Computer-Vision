{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "glii5UQrEZyo",
   "metadata": {
    "id": "glii5UQrEZyo"
   },
   "source": [
    "# Novel view synthesis with NeRF: Training and Testing on Colab\n",
    "\n",
    "If you're running this on Colab, insert the following Javascript snippet into your browser console so that your Colab runtime won't time out. Open developer-settings (in your web-browser) with Ctrl+Shift+I then click on console tab and type this on the console prompt. (for mac press Option+Command+I)\n",
    "\n",
    "(You can ignore this if you manually click connect button.)\n",
    "```Javascript\n",
    "function ClickConnect(){\n",
    "    console.log(\"Clicked on connect button\");\n",
    "    document.querySelector(\"colab-connect-button\").click()\n",
    "}\n",
    "setInterval(ClickConnect,60000)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c8b02-0b71-47c8-b821-c793c61e68db",
   "metadata": {
    "id": "ad2c8b02-0b71-47c8-b821-c793c61e68db"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio.v2 as imageio\n",
    "import time\n",
    "\n",
    "device_type = (\n",
    "    \"cuda\" if torch.cuda.is_available() else\n",
    "    \"mps\" if torch.backends.mps.is_available() else\n",
    "    \"cpu\"\n",
    ")\n",
    "device = torch.device(device_type)\n",
    "print(device)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A6AAydx1DIiL",
   "metadata": {
    "id": "A6AAydx1DIiL"
   },
   "outputs": [],
   "source": [
    "!unzip cv_proj6_colab.zip -d cv_proj6\n",
    "os.chdir('cv_proj6')\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TfTcInpuE07q",
   "metadata": {
    "id": "TfTcInpuE07q"
   },
   "source": [
    "# Part3 Train NeRF on 360 scene\n",
    "If you passed all tests, you can start training NeRF!\n",
    "Expect to reach PSNR greater than or equal to 20 after training for 1,000 iterations with num_encoding_functions=6.\n",
    "\n",
    "Small note: The training can get stuck at local optimum with certain initialization, in which can the PSNR will not improve with more training. You can rerun the code to restart training. Training 2000 steps takes ~20 mins using T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beba3db-87b5-4134-8487-3a122695d6a8",
   "metadata": {
    "id": "5beba3db-87b5-4134-8487-3a122695d6a8"
   },
   "outputs": [],
   "source": [
    "# Load input images, poses, and intrinsics\n",
    "data = np.load(\"lego_data_update.npz\")\n",
    "\n",
    "# Images\n",
    "images = data[\"images\"]\n",
    "\n",
    "# Height and width of each image\n",
    "height, width = images.shape[1:3]\n",
    "\n",
    "# Camera extrinsics (poses)\n",
    "tform_cam2world = data[\"poses\"]\n",
    "tform_cam2world = torch.from_numpy(tform_cam2world).to(device)\n",
    "\n",
    "# Camera intrinsics\n",
    "cam_intrinsics = data[\"intrinsics\"]\n",
    "cam_intrinsics = torch.from_numpy(cam_intrinsics).to(device)\n",
    "\n",
    "# Near and far clipping thresholds for depth values.\n",
    "near_thresh = 0.667\n",
    "far_thresh = 2.\n",
    "\n",
    "# Hold one image out (for test).\n",
    "testimg, testpose = images[101], tform_cam2world[101]\n",
    "testimg = torch.from_numpy(testimg).to(device)\n",
    "\n",
    "# Rest images form test set\n",
    "testset, testsetpose = images[101:], tform_cam2world[101:]\n",
    "testset = torch.from_numpy(testset).to(device)\n",
    "\n",
    "# Map images to device\n",
    "images = torch.from_numpy(images[:100, ..., :3]).to(device)\n",
    "\n",
    "plt.imshow(testimg.detach().cpu().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c18e15-d66a-40cf-9410-32f97760d821",
   "metadata": {
    "id": "11c18e15-d66a-40cf-9410-32f97760d821"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training NeRF\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "from vision.part3 import train_nerf\n",
    "\n",
    "num_iters = 1000\n",
    "depth_samples_per_ray = 64\n",
    "\n",
    "model, encode = train_nerf(\n",
    "    images, tform_cam2world, cam_intrinsics, testpose, testimg, height, width,\n",
    "    near_thresh, far_thresh, device='cuda', num_frequencies=6,\n",
    "    depth_samples_per_ray=depth_samples_per_ray, lr=5e-4, num_iters=num_iters, display_every=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvsCdZcvEFJB",
   "metadata": {
    "id": "GvsCdZcvEFJB"
   },
   "source": [
    "## Interactive visualization\n",
    "Training finished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cAFDRwLD9Ve",
   "metadata": {
    "id": "9cAFDRwLD9Ve"
   },
   "outputs": [],
   "source": [
    "from vision.part2 import render_image_nerf\n",
    "\n",
    "trans_t = lambda t : np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "rot_phi = lambda phi : np.array([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi), np.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "rot_theta = lambda th : np.array([\n",
    "    [np.cos(th),0,-np.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(th),0, np.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=np.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "    c2w = rot_theta(theta/180.*np.pi) @ c2w\n",
    "    c2w = np.array([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]]) @ c2w\n",
    "    c2w = c2w @ np.array([[1,0,0,0],[0,-1,0,0],[0,0,-1,0],[0,0,0,1]])\n",
    "    c2w = np.array(c2w, dtype=np.float32)\n",
    "    return c2w\n",
    "\n",
    "%matplotlib inline\n",
    "from ipywidgets import interactive, widgets\n",
    "def f(**kwargs):\n",
    "    render_poses = torch.from_numpy(pose_spherical(**kwargs)).to(device)\n",
    "    rgb_predicted, _ = render_image_nerf(height, width, cam_intrinsics,\n",
    "                                              render_poses[:3,:4], near_thresh,\n",
    "                                              far_thresh, depth_samples_per_ray,\n",
    "                                              encode, model)\n",
    "    img = np.clip(rgb_predicted.detach().cpu().numpy(),0,1)\n",
    "\n",
    "    plt.figure(2, figsize=(20,6))\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "sldr = lambda v, mi, ma: widgets.FloatSlider(\n",
    "    value=v,\n",
    "    min=mi,\n",
    "    max=ma,\n",
    "    step=.01,\n",
    ")\n",
    "\n",
    "names = [\n",
    "    ['theta', [100., 0., 360]],\n",
    "    ['phi', [-30., -90, 0]],\n",
    "    ['radius', [1.5, 1., 2.]],\n",
    "]\n",
    "\n",
    "interactive_plot = interactive(f, **{s[0] : sldr(*s[1]) for s in names})\n",
    "output = interactive_plot.children[-1]\n",
    "output.layout.height = '350px'\n",
    "interactive_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KwW25uXkENdm",
   "metadata": {
    "id": "KwW25uXkENdm"
   },
   "source": [
    "## Qualitative evaluation\n",
    "Create a 360 video with the trained NeRF by rendering a set of images around the object. Evaluate the novel view synthesis results qualitatively.\n",
    "***Save your video and push the video to your github repository.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XpRl76DeEK-5",
   "metadata": {
    "id": "XpRl76DeEK-5"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "# 360 video\n",
    "import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "  rgbs = []\n",
    "  for th in tqdm.tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
    "      render_poses = torch.from_numpy(pose_spherical(th, -30., 1.4)).to(device)\n",
    "      rgb_predicted, _ = render_image_nerf(height, width, cam_intrinsics,\n",
    "                                              render_poses[:3,:4], near_thresh,\n",
    "                                              far_thresh, depth_samples_per_ray,\n",
    "                                              encode, model)\n",
    "      rgbs.append(rgb_predicted.detach().cpu().numpy())\n",
    "rgbs = np.array(rgbs)\n",
    "print('Done, saving', rgbs.shape)\n",
    "moviebase = os.path.join('{}_spiral_{:06d}_'.format(\"lego\", num_iters))\n",
    "to8b = lambda x : (255*np.clip(x,0,1)).astype(np.uint8)\n",
    "imageio.mimwrite(moviebase + 'rgb.mp4', to8b(rgbs), fps=30, quality=8)\n",
    "\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "mp4 = open(moviebase + 'rgb.mp4','rb').read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "HTML(\"\"\"\n",
    "<video width=400 controls autoplay loop>\n",
    "      <source src=\"%s\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S9KFkN8yEQSU",
   "metadata": {
    "id": "S9KFkN8yEQSU"
   },
   "source": [
    "## Quantitive results\n",
    "Evaluate the novel view synthesis results quantitatively on the test set with PSNR metric. Expect to reach PSNR greater than or equal to 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UBgTZ5NOERpU",
   "metadata": {
    "id": "UBgTZ5NOERpU"
   },
   "outputs": [],
   "source": [
    "psnrs = []\n",
    "for i in range(testset.shape[0]):\n",
    "  with torch.no_grad():\n",
    "    rgb_predicted, depth_predicted = render_image_nerf(height, width, cam_intrinsics,\n",
    "                                            testsetpose[i], near_thresh,\n",
    "                                            far_thresh, depth_samples_per_ray,\n",
    "                                            encode, model)\n",
    "  loss = F.mse_loss(rgb_predicted, testset[i])\n",
    "  psnr = -10. * torch.log10(loss)\n",
    "  psnrs.append(psnr.item())\n",
    "print(\"PSNR on test set: %.2f\" % np.mean(np.array(psnrs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cead9611-06ba-474d-975a-dc8cce5032f8",
   "metadata": {
    "id": "esbpsdBwESwB"
   },
   "source": [
    "### Don't forget to download nerf_model.pth from colab and put it in the output/ directory in your local project folder before you zip for submission!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
